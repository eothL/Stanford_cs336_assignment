
This project CS336 Assignment 1 (Basics) implements the core building blocks of modern
transformer language models from scratch, including NN layers, attention mechanism, optimization algorithm, and tokenization. The assignment is structured around implementing various functions in the `adapters.py` file to pass comprehensive test suites. 

Environment:
- A dedicated env exists for the agent; prefer `python` (not `python3`) and use `uv` for package management.

Key code:
- tests/adapters.py: required functions (test targets).
- basic/Tokenizer.py: tokenizer class (currently stubs).
- basic/train_bpe.py: main BPE training logic (heap-based).
- basic/pretokenization.py: chunking + multiprocessing pretokenization.
- basic/assigment_question.py: assignment answers / training script.


Progress so far (Theo + prior agents):
- Implemented split-special-token pretokenization in `basic/pretokenization.py`:
  - chunk boundaries avoid splitting `<|endoftext|>` across workers
  - pattern excludes the split token; token counted explicitly.
- Contains debug scripts in `local/` or can be used to vibe test anything quickly. No restriction folder in term of implementation.
- Heap performance experiments done in `basic/train_bpe_copy.py` / `local/train_bpe_test.py`
  (stale-node rebuild heuristics, `heapq` variant, profiling).

Remaining work (use âœ…:done/ğŸŸ¡: currently doing/â³: not started):

1) Basic Neural Network Components
- â³ Linear (`run_linear`)
- â³ Embedding (`run_embedding`)
- â³ RMSNorm (`run_rmsnorm`)
- â³ SiLU (`run_silu`)

1) Attention Mechanisms
- â³ Scaled dotâ€‘product attention (`run_scaled_dot_product_attention`)
- â³ Multiâ€‘head selfâ€‘attention (`run_multihead_self_attention`)
- â³ RoPE (`run_rope`)
- â³ Multiâ€‘head selfâ€‘attention w/ RoPE (`run_multihead_self_attention_with_rope`)

1) Feedâ€‘Forward Networks
- â³ SwiGLU (`run_swiglu`)

1) Transformer Architecture
- â³ Transformer block (`run_transformer_block`)
- â³ Transformer LM (`run_transformer_lm`)

1) Training Infrastructure
- â³ Batch sampling (`run_get_batch`)
- â³ Softmax (`run_softmax`)
- â³ Crossâ€‘entropy (`run_cross_entropy`)
- â³ Gradient clipping (`run_gradient_clipping`)

1) Optimization
- â³ AdamW (`get_adamw_cls`)
- â³ Cosine LR schedule (`run_get_lr_cosine_schedule`)

1) Model Serialization
- â³ Save checkpoint (`run_save_checkpoint`)
- â³ Load checkpoint (`run_load_checkpoint`)

1) Tokenization
- ğŸŸ¡ Tokenizer class (`basic/Tokenizer.py`):
  - `from_files`, `encode`, `encode_iterable`, `decode`
- ğŸŸ¡ BPE training (heapâ€‘based) exists but needs final TinyStories run + output files

1) BPE Training Deliverables (TinyStories)
- ğŸŸ¡ Train on TinyStories, vocab=10,000, include `<|endoftext|>`
- ğŸŸ¡ Serialize vocab/merges (GPTâ€‘2 bytesâ†’unicode)
- ğŸŸ¡ Report training time + peak memory
- ğŸŸ¡ Identify longest token + comment if it makes sense
- ğŸŸ¡ Provide 1â€“2 sentence answer for profiling (part b)

